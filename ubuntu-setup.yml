---
- name: Setup Ubuntu 24 System
  hosts: localhost
  connection: local
  become: yes

  vars:
    ssh_port: 33412
    ssh_user: "{{ ansible_env.SUDO_USER | default(ansible_env.USER) }}"
    user_home: "{{ '/home/' + (ansible_env.SUDO_USER | default(ansible_env.USER)) }}"
    auto_reboot_time: "06:00"  # 6 AM daily reboot

    # Local installer storage for reproducible builds
    installers_base_path: "/opt/installers"

    # NVIDIA Driver configuration
    nvidia_driver_version: "580"
    nvidia_driver_version_full: "580.95.05"
    nvidia_driver_local_repo_url: "https://developer.download.nvidia.com/compute/nvidia-driver/{{ nvidia_driver_version_full }}/local_installers/nvidia-driver-local-repo-ubuntu2404-{{ nvidia_driver_version_full }}_1.0-1_amd64.deb"

    # CUDA Toolkit configuration
    cuda_version: "13-0"
    cuda_version_full: "13.0"
    cuda_version_full_numeric: "13.0.2"
    cuda_local_repo_url: "https://developer.download.nvidia.com/compute/cuda/{{ cuda_version_full_numeric }}/local_installers/cuda-repo-ubuntu2404-13-0-local_{{ cuda_version_full_numeric }}-580.95.05-1_amd64.deb"

    # TensorRT version configuration
    tensorrt_version_major: "10.13"     # Major version (e.g., "10.13")
    tensorrt_version_full: "10.13.3"    # Full version including point release (e.g., "10.13.3")
    tensorrt_version_pattern: "10.13.*" # Pattern for apt search

    # TensorRT installation method:
    # - "local": Download and use local repository (recommended - guarantees version availability)
    # - "network": Use CUDA network repository (only works while version available)
    # - "auto": Try network first, fall back to local if not found
    tensorrt_install_method: "local"

    # TensorRT local repo URL (auto-constructed if using local method)
    # Override this if you want to use a custom URL or local file path
    tensorrt_local_repo_url: ""
    realvnc_version: "7.13.0"
    # Data folders (data/jpg/video/...) created under this path; default is home so ~/data sits next to ~/code
    app_data_path: "{{ user_home }}"

    log_file: "/var/log/ansible-ubuntu-setup-{{ ansible_date_time.iso8601_basic_short }}.log"
    rtl8812au_usb_id: "0bda:8812"
    hwe_kernel_meta_package: "linux-generic-hwe-24.04"
    hwe_kernel_headers_meta_package: "linux-headers-generic-hwe-24.04"
    hwe_kernel_pin_enabled: true
    hwe_kernel_pin_version: "6.17.0-14"
    hwe_kernel_flavor: "generic"
    hwe_kernel_require_reboot_before_continue: true
    hwe_kernel_pinned_packages:
      - "linux-image-{{ hwe_kernel_pin_version }}-{{ hwe_kernel_flavor }}"
      - "linux-headers-{{ hwe_kernel_pin_version }}-{{ hwe_kernel_flavor }}"
      - "linux-modules-{{ hwe_kernel_pin_version }}-{{ hwe_kernel_flavor }}"
      - "linux-modules-extra-{{ hwe_kernel_pin_version }}-{{ hwe_kernel_flavor }}"

  tasks:

    # ==========================================================
    # 0. NETWORK INFO (for user to record)
    # ==========================================================
    - name: Gather network interface info for display
      shell: |
        echo "========== NETWORK INFO (record MACs and IPs) =========="
        ip -br addr show | while read iface state rest; do
          [ "$iface" = "lo" ] && continue
          mac=$(ip link show "$iface" 2>/dev/null | grep 'link/ether' | awk '{print $2}')
          [ -z "$mac" ] && mac="N/A"
          echo "  $iface: state=$state MAC=$mac"
          ip -4 addr show "$iface" 2>/dev/null | grep inet | awk '{print "    IP: "$2}'
        done
        echo "========================================================="
      register: network_info_display
      changed_when: false

    - name: Display network info to user
      debug:
        msg: "{{ network_info_display.stdout_lines }}"

    - name: Log that network info was displayed
      shell: "echo 'Network info (MACs, IPs) displayed to user at start' >> {{ log_file }}"
      changed_when: false

    # ==========================================================
    # 1. INITIALIZE LOG FILE
    # ==========================================================
    - name: Create log directory
      file:
        path: /var/log
        state: directory
        mode: '0755'
    
    - name: Initialize log file with header
      copy:
        dest: "{{ log_file }}"
        content: |
          ===================================================================
          Ubuntu 24 System Setup - Started at {{ ansible_date_time.iso8601 }}
          Host: {{ inventory_hostname }}
          ===================================================================

        mode: '0644'
      tags: always

    # ==========================================================
    # 1. PRE-FLIGHT VALIDATION
    # ==========================================================
    - name: Check available disk space on /opt
      shell: df -BG /opt | tail -1 | awk '{print $4}' | sed 's/G//'
      register: opt_free_space
      changed_when: false
      failed_when: false

    - name: Fail if insufficient disk space
      fail:
        msg: "Insufficient disk space on /opt. Available: {{ opt_free_space.stdout }}GB, Required: 5GB minimum for installers"
      when: opt_free_space.stdout | int < 5

    - name: Test network connectivity to NVIDIA
      uri:
        url: "https://developer.download.nvidia.com/"
        method: HEAD
        timeout: 10
        status_code: [200, 301, 302, 403]
      register: nvidia_network_check
      failed_when: false
      changed_when: false

    - name: Test network connectivity to CUDA repos
      uri:
        url: "https://developer.download.nvidia.com/compute/cuda/"
        method: HEAD
        timeout: 10
        status_code: [200, 301, 302, 403]
      register: cuda_network_check
      failed_when: false
      changed_when: false

    - name: Log pre-flight validation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "1. PRE-FLIGHT VALIDATION - {{ ansible_date_time.iso8601 }}" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "Available disk space on /opt: {{ opt_free_space.stdout }}GB" >> {{ log_file }}
        echo "Network connectivity to NVIDIA: {{ 'OK' if nvidia_network_check.status is defined else 'FAILED (offline mode?)' }}" >> {{ log_file }}
        echo "Network connectivity to CUDA repos: {{ 'OK' if cuda_network_check.status is defined else 'FAILED (offline mode?)' }}" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - debug:
        msg: "âœ… Pre-flight checks passed: {{ opt_free_space.stdout }}GB available, network: {{ 'online' if nvidia_network_check.status is defined else 'offline' }}"

    # ==========================================================
    # 2. HWE KERNEL BASELINE
    # ==========================================================
    - name: Install Ubuntu 24.04 HWE kernel meta packages
      apt:
        update_cache: yes
        name:
          - "{{ hwe_kernel_meta_package }}"
          - "{{ hwe_kernel_headers_meta_package }}"
        state: present
      register: hwe_kernel_meta_install

    - name: Install pinned HWE kernel package set
      apt:
        name: "{{ hwe_kernel_pinned_packages }}"
        state: present
      register: hwe_kernel_pin_install
      when: hwe_kernel_pin_enabled | bool

    - name: Hold pinned HWE kernel packages
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop: "{{ hwe_kernel_pinned_packages }}"
      when: hwe_kernel_pin_enabled | bool

    - name: Set kernel baseline facts
      set_fact:
        hwe_target_kernel_release: "{{ hwe_kernel_pin_version }}-{{ hwe_kernel_flavor }}"
        hwe_kernel_active_matches_target: "{{ ansible_kernel == (hwe_kernel_pin_version ~ '-' ~ hwe_kernel_flavor) }}"
        hwe_kernel_native_8812au_supported: "{{ (ansible_kernel.split('-')[0]) is version('6.13', '>=') }}"

    - name: Log kernel baseline status
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "2. HWE KERNEL BASELINE - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "Running kernel: {{ ansible_kernel }}" >> {{ log_file }}
        echo "HWE meta packages installed: {{ hwe_kernel_meta_install.changed }}" >> {{ log_file }}
        {% if hwe_kernel_pin_enabled %}
        echo "Pinned kernel target: {{ hwe_target_kernel_release }}" >> {{ log_file }}
        echo "Pinned kernel packages updated: {{ hwe_kernel_pin_install.changed | default(false) }}" >> {{ log_file }}
        echo "Pinned kernel active now: {{ hwe_kernel_active_matches_target }}" >> {{ log_file }}
        {% endif %}
        echo "RTL8812AU native support available on running kernel: {{ hwe_kernel_native_8812au_supported }}" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - name: Display kernel baseline reminder when reboot is needed
      debug:
        msg: |
          Kernel baseline packages are installed, but the running kernel is still {{ ansible_kernel }}.
          Reboot before relying on native RTL8812AU support (target: {{ hwe_target_kernel_release }}).
      when:
        - hwe_kernel_pin_enabled | bool
        - not (hwe_kernel_active_matches_target | bool)

    # ==========================================================
    # 3. KERNEL REBOOT GATE (early exit)
    # ==========================================================
    - name: Stop run until pinned kernel is active
      fail:
        msg: |
          Kernel baseline is installed but not active yet.
          Running kernel: {{ ansible_kernel }}
          Required kernel: {{ hwe_target_kernel_release }}
          Reboot now (sudo reboot), then re-run this playbook.
      when:
        - hwe_kernel_pin_enabled | bool
        - hwe_kernel_require_reboot_before_continue | bool
        - not (hwe_kernel_active_matches_target | bool)

    # ==========================================================
    # 4. PROMPTS (after kernel baseline gate)
    # ==========================================================
    - name: Prompt for Healthchecks.io URL
      pause:
        prompt: |

          ============================================================
          HEALTHCHECKS.IO SETUP
          ============================================================

          Please create a healthcheck at https://healthchecks.io

          Steps:
          1. Go to https://healthchecks.io and create a free account
          2. Create a new check with name: "{{ ansible_hostname }}"
          3. Set the period to 10 minutes (allows 5 min pings + buffer)
          4. Copy the ping URL (looks like: https://hc-ping.com/YOUR-UUID-HERE)

          Paste your Healthchecks.io ping URL (or press Enter to skip)
      register: healthchecks_prompt
      when: healthchecks_url is not defined

    - name: Set healthchecks URL from user input or default
      set_fact:
        healthchecks_url: "{{ healthchecks_prompt.user_input if (healthchecks_prompt.user_input | default('') | length > 0) else 'SKIPPED' }}"
      when: healthchecks_url is not defined

    - name: Set healthchecks URL if already defined
      set_fact:
        healthchecks_url: "{{ healthchecks_url }}"
      when: healthchecks_url is defined

    - name: Prompt for boot mode
      pause:
        prompt: |

          ============================================================
          BOOT MODE
          ============================================================
          Do you want:
            1) GNOME to open on boot (full desktop)
            2) Minimal X / king_detector approach (boot to server, no GNOME; xdotool-style)
          Enter 1 or 2
      register: boot_mode_prompt

    - name: Set boot mode fact
      set_fact:
        boot_mode: "{{ 'gnome' if boot_mode_prompt.user_input | default('1') | trim == '1' else 'minimal_x' }}"

    - name: Prompt for deployment mode
      pause:
        prompt: |

          ============================================================
          DEPLOYMENT MODE
          ============================================================
          Choose mode:
            1) Production (WiFi hardware required)
            2) Test/check (WiFi hardware may be absent)
          Enter 1 or 2
      register: deployment_mode_prompt

    - name: Set deployment mode fact
      set_fact:
        deployment_mode: "{{ 'production' if deployment_mode_prompt.user_input | default('1') | trim != '2' else 'test' }}"

    # ==========================================================
    # 5. WIFI READINESS (early gate before the rest of setup)
    # ==========================================================
    - name: Install WiFi detection tools
      apt:
        name:
          - usbutils
          - pciutils
        state: present

    - name: Detect expected RTL8812AU adapter
      shell: lsusb | grep -qi "{{ rtl8812au_usb_id }}"
      register: wifi_expected_usb
      changed_when: false
      failed_when: false

    - name: Detect other likely USB WiFi adapters
      shell: |
        lsusb | grep -Eiv "{{ rtl8812au_usb_id }}" | grep -Ei 'Wireless|WiFi|WLAN|802\.11|Atheros|Ralink|MediaTek|Qualcomm|Broadcom|Realtek.*(881|882|8723|88)'
      register: wifi_other_usb
      changed_when: false
      failed_when: false

    - name: Detect PCIe wireless adapters
      shell: lspci -nn | grep -Ei 'Network controller|Wireless|802\.11'
      register: wifi_pci
      changed_when: false
      failed_when: false

    - name: Detect currently working wireless interfaces
      shell: |
        for iface in /sys/class/net/*; do
          iface_name="$(basename "$iface")"
          if [ -d "/sys/class/net/${iface_name}/wireless" ]; then
            echo "${iface_name}"
          fi
        done
      register: wifi_wireless_ifaces
      changed_when: false
      failed_when: false

    - name: Set WiFi classification facts
      set_fact:
        rtl8812au_present: "{{ wifi_expected_usb.rc == 0 }}"
        wifi_other_adapter_detected: "{{ (wifi_other_usb.stdout | default('') | trim | length > 0) or (wifi_pci.stdout | default('') | trim | length > 0) }}"
        wifi_native_working: "{{ (wifi_wireless_ifaces.stdout_lines | default([]) | length) > 0 }}"
        rtl8812au_install_status: "not_attempted"

    - name: Compute WiFi decision summary
      set_fact:
        wifi_kernel_supports_native_8812au: "{{ (ansible_kernel.split('-')[0]) is version('6.13', '>=') }}"
        wifi_path: "{{ 'expected_adapter' if (rtl8812au_present | bool) else ('other_adapter' if (wifi_other_adapter_detected | bool) else 'no_adapter') }}"
        wifi_proposed_action: >-
          {% if (rtl8812au_present | bool) and (wifi_kernel_supports_native_8812au | bool) and (wifi_native_working | bool) %}
          Use native in-kernel RTL8812AU support (rtw88_8812au); no external DKMS driver install.
          {% elif (rtl8812au_present | bool) and (wifi_kernel_supports_native_8812au | bool) and (not wifi_native_working | bool) %}
          RTL8812AU is present and kernel supports it, but no wireless interface is active; fail in production.
          {% elif (rtl8812au_present | bool) and (not wifi_kernel_supports_native_8812au | bool) %}
          Running kernel is too old for native RTL8812AU support; reboot into pinned HWE kernel ({{ hwe_target_kernel_release }}) or fail in production.
          {% elif (wifi_other_adapter_detected | bool) and (wifi_native_working | bool) %}
          Keep native driver (different adapter appears functional); no custom driver install.
          {% elif (wifi_other_adapter_detected | bool) and (not wifi_native_working | bool) %}
          Different adapter detected but no native WiFi interface found; treat as unsupported and fail in production.
          {% else %}
          No WiFi adapter detected; fail in production and continue in test/check mode.
          {% endif %}

    - name: Confirm WiFi approach with operator
      pause:
        prompt: |

          ============================================================
          WIFI READINESS DECISION
          ============================================================
          Deployment mode: {{ deployment_mode }}
          Running kernel: {{ ansible_kernel }}
          Native RTL8812AU support on running kernel: {{ wifi_kernel_supports_native_8812au }}
          Expected adapter ({{ rtl8812au_usb_id }}): {{ 'present' if rtl8812au_present else 'absent' }}
          Other adapter detected: {{ wifi_other_adapter_detected }}
          Working wireless interfaces: {{ (wifi_wireless_ifaces.stdout_lines | default([]) | join(', ')) if (wifi_wireless_ifaces.stdout_lines | default([]) | length > 0) else 'none' }}

          Proposed action:
          {{ wifi_proposed_action }}

          Choose:
            1) Proceed with proposed action
            2) Skip WiFi changes/check enforcement for this run
            3) Abort run now
          Enter 1, 2, or 3
      register: wifi_plan_prompt

    - name: Set WiFi operator decision
      set_fact:
        wifi_operator_decision: "{{ (wifi_plan_prompt.user_input | default('1') | trim) if (wifi_plan_prompt.user_input | default('1') | trim) in ['1', '2', '3'] else '1' }}"

    - name: Abort on operator request
      fail:
        msg: "Operator aborted during WiFi readiness decision."
      when: wifi_operator_decision == '3'

    - name: Log WiFi readiness decision
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "4. WIFI READINESS - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "Deployment mode: {{ deployment_mode }}" >> {{ log_file }}
        echo "Running kernel: {{ ansible_kernel }}" >> {{ log_file }}
        echo "Native RTL8812AU support on running kernel: {{ wifi_kernel_supports_native_8812au }}" >> {{ log_file }}
        echo "Expected adapter ({{ rtl8812au_usb_id }}): {{ rtl8812au_present }}" >> {{ log_file }}
        echo "Other adapter detected: {{ wifi_other_adapter_detected }}" >> {{ log_file }}
        echo "Wireless interfaces: {{ (wifi_wireless_ifaces.stdout_lines | default([]) | join(', ')) if (wifi_wireless_ifaces.stdout_lines | default([]) | length > 0) else 'none' }}" >> {{ log_file }}
        echo "Proposed action: {{ wifi_proposed_action | regex_replace('\\n', ' ') }}" >> {{ log_file }}
        echo "Operator decision: {{ wifi_operator_decision }}" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - name: Enforce production policy for missing WiFi adapter
      fail:
        msg: "No WiFi adapter detected. Production mode requires a WiFi adapter."
      when:
        - wifi_operator_decision == '1'
        - deployment_mode == 'production'
        - wifi_path == 'no_adapter'

    - name: Enforce production policy for unsupported non-native adapter
      fail:
        msg: "A non-RTL8812AU adapter was detected but no native wireless interface is available. Production mode requires a working WiFi adapter."
      when:
        - wifi_operator_decision == '1'
        - deployment_mode == 'production'
        - wifi_path == 'other_adapter'
        - not (wifi_native_working | bool)

    - name: Enforce production policy when RTL8812AU adapter is not working natively
      fail:
        msg: "RTL8812AU adapter was detected but no native wireless interface is active on kernel {{ ansible_kernel }}. Reboot into {{ hwe_target_kernel_release }} or fix adapter state."
      when:
        - wifi_operator_decision == '1'
        - deployment_mode == 'production'
        - wifi_path == 'expected_adapter'
        - not (wifi_native_working | bool)

    - name: Warn when RTL8812AU adapter is present but not active (test mode)
      debug:
        msg: "RTL8812AU adapter is present but no wireless interface is active on kernel {{ ansible_kernel }}. Continuing because deployment mode is test/check."
      when:
        - wifi_operator_decision == '1'
        - deployment_mode == 'test'
        - wifi_path == 'expected_adapter'
        - not (wifi_native_working | bool)

    - name: Warn about non-native adapter without working wireless interface (test mode)
      debug:
        msg: "Non-RTL8812AU adapter detected, but no wireless interface is up natively. Continuing because deployment mode is test/check."
      when:
        - wifi_operator_decision == '1'
        - deployment_mode == 'test'
        - wifi_path == 'other_adapter'
        - not (wifi_native_working | bool)

    - name: Mark WiFi status when RTL8812AU adapter is working natively
      set_fact:
        rtl8812au_install_status: "native_expected_adapter_ok"
      when:
        - wifi_operator_decision == '1'
        - wifi_path == 'expected_adapter'
        - wifi_native_working | bool

    - name: Mark WiFi status when RTL8812AU adapter is present but not active (test mode)
      set_fact:
        rtl8812au_install_status: "expected_adapter_not_working_test"
      when:
        - wifi_operator_decision == '1'
        - deployment_mode == 'test'
        - wifi_path == 'expected_adapter'
        - not (wifi_native_working | bool)

    - name: Mark WiFi status when using native non-RTL adapter
      set_fact:
        rtl8812au_install_status: "native_adapter_ok"
      when:
        - wifi_operator_decision == '1'
        - wifi_path == 'other_adapter'
        - wifi_native_working | bool

    - name: Mark WiFi status when no adapter is acceptable in test mode
      set_fact:
        rtl8812au_install_status: "no_adapter_allowed_test"
      when:
        - wifi_operator_decision == '1'
        - deployment_mode == 'test'
        - wifi_path == 'no_adapter'

    - name: Mark WiFi status when non-native adapter is not working (test mode)
      set_fact:
        rtl8812au_install_status: "other_adapter_not_working_test"
      when:
        - wifi_operator_decision == '1'
        - deployment_mode == 'test'
        - wifi_path == 'other_adapter'
        - not (wifi_native_working | bool)

    - name: Mark WiFi status when operator skipped WiFi handling
      set_fact:
        rtl8812au_install_status: "skipped_by_operator"
      when: wifi_operator_decision == '2'

    - name: Report WiFi readiness outcome
      debug:
        msg: "WiFi readiness outcome: {{ rtl8812au_install_status }}"

    # ==========================================================
    # 5. CREATE LOCAL INSTALLER STORAGE
    # ==========================================================
    - name: Create local installer base directory
      file:
        path: "{{ installers_base_path }}"
        state: directory
        mode: '0755'
        owner: root
        group: root

    - name: Create nvidia-driver installer directory
      file:
        path: "{{ installers_base_path }}/nvidia-driver"
        state: directory
        mode: '0755'

    - name: Create cuda installer directory
      file:
        path: "{{ installers_base_path }}/cuda"
        state: directory
        mode: '0755'

    - name: Create tensorrt installer directory
      file:
        path: "{{ installers_base_path }}/tensorrt"
        state: directory
        mode: '0755'

    - name: Log local installer storage creation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "5. LOCAL INSTALLER STORAGE - {{ ansible_date_time.iso8601 }}" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "Created {{ installers_base_path }} for reproducible builds" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - name: Create version manifest
      copy:
        dest: "{{ installers_base_path }}/VERSIONS.json"
        mode: '0644'
        content: |
          {
            "manifest_created": "{{ ansible_date_time.iso8601 }}",
            "hostname": "{{ ansible_hostname }}",
            "ubuntu_version": "{{ ansible_distribution_version }}",
            "components": {
              "nvidia_driver": {
                "version": "{{ nvidia_driver_version_full }}",
                "method": "local_repository",
                "url": "{{ nvidia_driver_local_repo_url }}"
              },
              "cuda_toolkit": {
                "version": "{{ cuda_version_full_numeric }}",
                "method": "local_repository",
                "url": "{{ cuda_local_repo_url }}"
              },
              "tensorrt": {
                "version": "{{ tensorrt_version_full }}",
                "method": "{{ tensorrt_install_method }}",
                "url": "{{ tensorrt_local_repo_url if tensorrt_local_repo_url else 'auto-constructed' }}"
              },
              "pytorch_cuda": {
                "version": "cu130",
                "note": "Installed via pip in post-reboot-verify.yml"
              }
            },
            "reproducibility": {
              "local_installers": "{{ installers_base_path }}",
              "package_holding": "dpkg hold on nvidia-*-{{ nvidia_driver_version }}, cuda-*-{{ cuda_version }}, tensorrt-*",
              "unattended_upgrades_blacklist": "/etc/apt/apt.conf.d/51nvidia-blacklist",
              "apt_preferences": "/etc/apt/preferences.d/99-gpu-stack-pinning"
            }
          }

    - debug:
        msg: "ðŸ“‹ Version manifest created at {{ installers_base_path }}/VERSIONS.json"

    - name: Create SHA-256 checksums placeholder file
      copy:
        dest: "{{ installers_base_path }}/SHA256SUMS"
        mode: '0644'
        content: |
          # SHA-256 Checksums for GPU Stack Installers
          # Generated: {{ ansible_date_time.iso8601 }}
          # Verify with: cd /opt/installers && sha256sum -c SHA256SUMS
          #
          # Checksums will be appended during installation
        force: no

    # ==========================================================
    # 6. SYSTEM UPDATE
    # ==========================================================
    - name: Update apt cache and upgrade packages
      apt:
        update_cache: yes
        cache_valid_time: 3600
        upgrade: dist
      register: apt_upgrade
    
    - name: Log system update
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "6. SYSTEM UPDATE - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "âœ… System updated successfully." >> {{ log_file }}
        echo "" >> {{ log_file }}
    
    - debug: msg="âœ… System updated successfully."

    # ==========================================================
    # 7. COMMON UTILITIES & MONITORING TOOLS
    # ==========================================================
    - name: Install general developer and monitoring tools
      apt:
        name:
          - build-essential
          - git
          - cmake
          - unzip
          - zip
          - wget
          - curl
          - vim
          - nano
          - tree
          - jq
          - netcat-traditional
          - psmisc
          - htop
          - btop
          - bmon
          - iftop
          - nvtop
          - wavemon
          - net-tools
          - lm-sensors
          - speedtest-cli
        state: present
      register: tools_install
    
    - name: Print versions of installed core tools
      shell: |
        echo "htop: $(htop --version | head -n1)"
        echo "git: $(git --version)"
        echo "cmake: $(cmake --version | head -n1)"
      register: base_versions
    
    - name: Log tools installation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "7. COMMON UTILITIES & MONITORING TOOLS - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        {{ base_versions.stdout_lines | map('regex_replace', '^', 'echo "') | map('regex_replace', '$', '" >> {{ log_file }}') | join('\n') }}
        echo "" >> {{ log_file }}
    
    - debug: msg="{{ base_versions.stdout_lines }}"

    - name: Install xdotool and x11-xserver-utils for minimal X / king_detector
      apt: { name: [xdotool, x11-xserver-utils], state: present }
      when: boot_mode == 'minimal_x'

    # ==========================================================
    # 3. SSH CONFIGURATION
    # ==========================================================
    - name: Install and configure OpenSSH server
      apt: { name: openssh-server, state: present }

    - name: Ensure .ssh directory exists for user
      file:
        path: "{{ user_home }}/.ssh"
        state: directory
        mode: '0700'
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"

    - name: Check if ssh-public-keys.txt exists in repo
      stat:
        path: "{{ playbook_dir }}/ssh-public-keys.txt"
      register: ssh_keys_file
      delegate_to: localhost
      run_once: true

    - name: Read SSH keys from file
      slurp:
        src: "{{ playbook_dir }}/ssh-public-keys.txt"
      register: ssh_keys_content
      when: ssh_keys_file.stat.exists | default(false)

    - name: Deploy SSH keys from ssh-public-keys.txt to authorized_keys
      authorized_key:
        user: "{{ ssh_user }}"
        key: "{{ item }}"
        state: present
      loop: "{{ (ssh_keys_content.content | default('') | b64decode).split('\n') | map('trim') | select() | select('search', '^ssh-') | list }}"
      when:
        - ssh_keys_content.content is defined
        - item | length > 0

    - name: Set SSH Port in sshd_config (single place)
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#?\s*Port\s+'
        line: "Port {{ ssh_port }}"
      notify: restart sshd

    - name: Set PasswordAuthentication no in sshd_config
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#?\s*PasswordAuthentication\s+'
        line: "PasswordAuthentication no"
      notify: restart sshd

    - name: Set PubkeyAuthentication yes in sshd_config
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#?\s*PubkeyAuthentication\s+'
        line: "PubkeyAuthentication yes"
      notify: restart sshd

    - name: Set PermitRootLogin no in sshd_config
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#?\s*PermitRootLogin\s+'
        line: "PermitRootLogin no"
      notify: restart sshd

    - name: Validate sshd_config
      command: /usr/sbin/sshd -t -f /etc/ssh/sshd_config
      changed_when: false

    - name: Log SSH configuration
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "3. SSH CONFIGURATION - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "âœ… SSH configured on port {{ ssh_port }} (key-only auth)" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - debug: msg="âœ… SSH configured on port {{ ssh_port }}"

    # ==========================================================
    # 4. FIREWALL
    # ==========================================================
    - name: Install and enable UFW
      apt: { name: ufw, state: present }
    - name: Allow SSH through UFW
      ufw: { rule: allow, port: "{{ ssh_port }}", proto: tcp }
    - name: Enable UFW non-interactively
      command: ufw --force enable
      args: { creates: /etc/ufw/ufw.conf }
      register: ufw_enable
    
    - name: Log firewall configuration
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "4. FIREWALL - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "âœ… UFW enabled with SSH port {{ ssh_port }} open." >> {{ log_file }}
        echo "" >> {{ log_file }}
    
    - debug: msg="âœ… UFW enabled with SSH port {{ ssh_port }} open."

    # ==========================================================
    # 5b. DATA FOLDERS (videos and jpgs for app)
    # ==========================================================
    - name: Ensure app data base path exists
      file:
        path: "{{ app_data_path }}"
        state: directory
        mode: '0755'
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"

    - name: Create app data directory structure
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"
      loop:
        - "{{ app_data_path }}/data"
        - "{{ app_data_path }}/data/jpg"
        - "{{ app_data_path }}/data/video"
        - "{{ app_data_path }}/data/jpg/no_hook"
        - "{{ app_data_path }}/data/jpg/no_overlay"

    - name: Log data folders creation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "5b. DATA FOLDERS - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "âœ… Created data/jpg/video structure at {{ app_data_path }}/data (next to code)" >> {{ log_file }}
        echo "" >> {{ log_file }}

    # ==========================================================
    # 5c. SCHEDULED REBOOT (root cron at 6 and 18)
    # ==========================================================
    - name: Install root cron for scheduled reboot at 6 and 18
      cron:
        name: "Scheduled reboot at 6 and 18"
        user: root
        minute: "0"
        hour: "6,18"
        job: /sbin/reboot

    - name: Install root cron to log cron execution (6 and 18)
      cron:
        name: "Cron test log at 6 and 18"
        user: root
        minute: "0"
        hour: "6,18"
        job: 'echo "Cron executed at $(date)" >> /var/log/cron_test.log'

    - name: Log scheduled reboot cron
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "5c. SCHEDULED REBOOT (CRON) - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "âœ… Root crontab: reboot and log at 06:00 and 18:00" >> {{ log_file }}
        echo "" >> {{ log_file }}

    # ==========================================================
    # 6. TAILSCALE
    # ==========================================================
    - name: Setup Tailscale repository and install
      block:
        - name: Create keyrings directory for Tailscale
          file:
            path: /usr/share/keyrings
            state: directory
            mode: '0755'

        - name: Download Tailscale GPG key
          get_url:
            url: https://pkgs.tailscale.com/stable/ubuntu/noble.noarmor.gpg
            dest: /usr/share/keyrings/tailscale-archive-keyring.gpg
            mode: '0644'

        - name: Download Tailscale repository list
          get_url:
            url: https://pkgs.tailscale.com/stable/ubuntu/noble.tailscale-keyring.list
            dest: /etc/apt/sources.list.d/tailscale.list
            mode: '0644'

        - name: Update apt cache for Tailscale
          apt:
            update_cache: yes

        - name: Install Tailscale
          apt:
            name: tailscale
            state: present

        - name: Enable and start tailscaled service
          systemd:
            name: tailscaled
            enabled: yes
            state: started

        - shell: echo "âœ… Tailscale installed. Run 'sudo tailscale up' to authenticate." >> {{ log_file }}
      rescue:
        - debug: msg="âš ï¸ Tailscale repo or install failed."
        - shell: echo "âš ï¸ Tailscale repo or install failed." >> {{ log_file }}
    
    - name: Log Tailscale installation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "6. TAILSCALE - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
    
    - debug: msg="âœ… Tailscale installed. Run 'sudo tailscale up' to authenticate."

    # ==========================================================
    # 7. REALVNC SERVER
    # ==========================================================
    - name: Install RealVNC
      block:
        - get_url:
            url: "https://downloads.realvnc.com/download/file/vnc.files/VNC-Server-{{ realvnc_version }}-Linux-x64.deb"
            dest: /tmp/realvnc-server.deb
            timeout: 60
        - apt: { deb: /tmp/realvnc-server.deb }
        - shell: echo "âœ… RealVNC {{ realvnc_version }} installed." >> {{ log_file }}
        - debug: msg="âœ… RealVNC {{ realvnc_version }} installed."
      rescue:
        - debug: msg="âš ï¸ RealVNC download failed."
        - shell: echo "âš ï¸ RealVNC download failed." >> {{ log_file }}
    
    - name: Log RealVNC installation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "7. REALVNC SERVER - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "" >> {{ log_file }}

    # ==========================================================
    # 8. VISUAL STUDIO CODE
    # ==========================================================
    - name: Install Visual Studio Code
      block:
        - name: Create keyrings directory for VS Code
          file:
            path: /usr/share/keyrings
            state: directory
            mode: '0755'

        - name: Download Microsoft GPG key
          get_url:
            url: https://packages.microsoft.com/keys/microsoft.asc
            dest: /tmp/microsoft.asc
            mode: '0644'

        - name: Install Microsoft GPG key
          shell: gpg --dearmor < /tmp/microsoft.asc > /usr/share/keyrings/microsoft.gpg
          args:
            creates: /usr/share/keyrings/microsoft.gpg

        - name: Add VS Code repository
          apt_repository:
            repo: "deb [arch=amd64 signed-by=/usr/share/keyrings/microsoft.gpg] https://packages.microsoft.com/repos/vscode stable main"
            state: present
            filename: vscode
            update_cache: yes

        - name: Install VS Code package
          apt:
            name: code
            state: present

        - shell: echo "âœ… VS Code installed." >> {{ log_file }}
        - debug: msg="âœ… VS Code installed."
      rescue:
        - debug: msg="âš ï¸ VS Code repo or install failed."
        - shell: echo "âš ï¸ VS Code repo or install failed." >> {{ log_file }}
    
    - name: Log VS Code installation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "8. VISUAL STUDIO CODE - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "" >> {{ log_file }}

    # ==========================================================
    # 9. DISPLAY SERVER (XORG) & VM DETECTION
    # ==========================================================
    - name: Detect if running in a virtual machine
      shell: systemd-detect-virt
      register: virt_detect
      failed_when: false
      changed_when: false

    - name: Set VM detection fact
      set_fact:
        is_virtual_machine: "{{ virt_detect.rc == 0 and virt_detect.stdout != 'none' }}"
        virtualization_type: "{{ virt_detect.stdout | default('none') }}"

    - name: Check if GDM3 is installed
      stat:
        path: /etc/gdm3/custom.conf
      register: gdm3_config

    - name: Check if LightDM is installed
      stat:
        path: /etc/lightdm/lightdm.conf
      register: lightdm_config

    - name: Disable Wayland and force Xorg (GDM3)
      lineinfile:
        path: /etc/gdm3/custom.conf
        regexp: '^#?WaylandEnable='
        line: 'WaylandEnable=false'
        insertafter: '^\[daemon\]'
        backup: yes
      notify: restart gdm3
      register: xorg_config_gdm3
      when: gdm3_config.stat.exists

    - name: Configure LightDM to use Xorg (if present)
      lineinfile:
        path: /etc/lightdm/lightdm.conf
        regexp: '^#?user-session='
        line: 'user-session=xubuntu'
        insertafter: '^\[Seat:\*\]'
        backup: yes
      notify: restart lightdm
      register: xorg_config_lightdm
      when: lightdm_config.stat.exists

    - name: Log display manager configuration
      shell: |
        {% if gdm3_config.stat.exists %}
        echo "âœ… GDM3 configured to use Xorg (Wayland disabled)" >> {{ log_file }}
        {% elif lightdm_config.stat.exists %}
        echo "âœ… LightDM configured to use Xorg" >> {{ log_file }}
        {% else %}
        echo "â„¹ï¸ No display manager (GDM3/LightDM) detected - skipping Wayland/Xorg configuration" >> {{ log_file }}
        {% endif %}

    - name: Apply VM-specific display fix (QXL)
      block:
        - name: Create X11 config directory
          file:
            path: /etc/X11/xorg.conf.d
            state: directory
            mode: '0755'

        - name: Configure QXL display for VMs
          copy:
            dest: /etc/X11/xorg.conf.d/10-qxl-display.conf
            mode: '0644'
            content: |
              Section "ServerFlags"
                  Option "AutoAddGPU" "false"
              EndSection

              Section "ServerLayout"
                  Identifier "Layout0"
                  Screen 0 "Screen0"
              EndSection

              Section "Device"
                  Identifier "QXL"
                  Driver "qxl"
                  BusID "PCI:0:1:0"
              EndSection

              Section "Screen"
                  Identifier "Screen0"
                  Device "QXL"
              EndSection
          notify:
            - restart gdm3
            - restart lightdm

        - name: Remove conflicting NVIDIA X11 config
          file:
            path: /etc/X11/xorg.conf.d/10-nvidia.conf
            state: absent
          notify:
            - restart gdm3
            - restart lightdm

        - shell: |
            echo "âœ… VM detected ({{ virtualization_type }}): Applied QXL display fix" >> {{ log_file }}

        - debug:
            msg: "âœ… VM detected ({{ virtualization_type }}): Applied QXL display fix"

      when: is_virtual_machine

    - name: Log display server configuration
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "9. DISPLAY SERVER (XORG) & VM DETECTION - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "System type: {{ 'Virtual Machine (' + virtualization_type + ')' if is_virtual_machine else 'Physical/Bare Metal' }}" >> {{ log_file }}
        echo "âœ… Display configured for Xorg (Wayland disabled)." >> {{ log_file }}
        echo "" >> {{ log_file }}

    - debug:
        msg: |
          âœ… Display configured for Xorg (Wayland disabled)
          System: {{ 'Virtual Machine (' + virtualization_type + ')' if is_virtual_machine else 'Physical/Bare Metal' }}

    # ==========================================================
    # 10. NVIDIA DRIVER & GRUB CONFIG (LOCAL REPO)
    # ==========================================================
    - name: Install NVIDIA driver from local repository
      block:
        - name: Download NVIDIA driver local repository package
          get_url:
            url: "{{ nvidia_driver_local_repo_url }}"
            dest: "{{ installers_base_path }}/nvidia-driver/nvidia-driver-local-repo.deb"
            mode: '0644'
            timeout: 300
          register: nvidia_driver_download

        - name: Calculate SHA-256 checksum of NVIDIA driver package
          shell: sha256sum {{ installers_base_path }}/nvidia-driver/nvidia-driver-local-repo.deb | awk '{print $1}'
          register: nvidia_driver_sha256
          changed_when: false
          when: nvidia_driver_download.changed

        - name: Log NVIDIA driver SHA-256 checksum
          shell: |
            echo "NVIDIA Driver SHA-256: {{ nvidia_driver_sha256.stdout }}" >> {{ log_file }}
            echo "  File: nvidia-driver-local-repo.deb" >> {{ log_file }}
            echo "  Downloaded: {{ ansible_date_time.iso8601 }}" >> {{ log_file }}
            echo "{{ nvidia_driver_sha256.stdout }}  nvidia-driver/nvidia-driver-local-repo.deb" >> {{ installers_base_path }}/SHA256SUMS
          when: nvidia_driver_download.changed

        - name: Install NVIDIA driver local repository package
          apt:
            deb: "{{ installers_base_path }}/nvidia-driver/nvidia-driver-local-repo.deb"
            state: present

        - name: Find NVIDIA driver repository directory
          shell: find /var -maxdepth 1 -type d -name 'nvidia-driver-local-repo-*' | head -n 1
          register: nvidia_repo_dir
          changed_when: false

        - name: Copy NVIDIA driver GPG key to keyrings
          shell: cp {{ nvidia_repo_dir.stdout }}/nvidia-*-keyring.gpg /usr/share/keyrings/
          args:
            creates: /usr/share/keyrings/nvidia-*-keyring.gpg
          when: nvidia_repo_dir.stdout != ""

        - name: Update apt cache with NVIDIA driver repository
          apt:
            update_cache: yes

        - name: Install NVIDIA driver meta-package
          apt:
            name: "nvidia-driver-{{ nvidia_driver_version }}"
            state: present
            update_cache: yes
          register: nvidia_driver_install

        - name: Configure GRUB for NVIDIA KMS
          lineinfile:
            path: /etc/default/grub
            regexp: '^GRUB_CMDLINE_LINUX_DEFAULT='
            line: 'GRUB_CMDLINE_LINUX_DEFAULT="quiet splash nvidia-drm.modeset=1"'
            backup: yes
          register: grub_updated

        - name: Update GRUB configuration
          command: update-grub
          when: grub_updated.changed

        - name: Configure NVIDIA modprobe options
          copy:
            dest: /etc/modprobe.d/nvidia.conf
            mode: '0644'
            content: |
              options nvidia-drm modeset=1
              options nvidia NVreg_PreserveVideoMemoryAllocations=1

        - shell: echo "âœ… NVIDIA driver {{ nvidia_driver_version_full }} installed from local repository." >> {{ log_file }}

    - name: Log NVIDIA driver installation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "10. NVIDIA DRIVER & GRUB CONFIG - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "Installed NVIDIA driver {{ nvidia_driver_version_full }} from local repository" >> {{ log_file }}
        echo "Stored in: {{ installers_base_path }}/nvidia-driver/" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - debug: msg="âœ… NVIDIA driver {{ nvidia_driver_version_full }} installed from local repository."

    # Hold NVIDIA driver packages to prevent upgrades
    - name: Get list of installed NVIDIA driver packages
      shell: dpkg -l | grep '^ii' | grep -E 'nvidia.*-{{ nvidia_driver_version }}' | awk '{print $2}'
      register: nvidia_packages_list
      changed_when: false
      failed_when: false

    - name: Hold NVIDIA driver packages
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop: "{{ nvidia_packages_list.stdout_lines }}"
      when: nvidia_packages_list.stdout_lines | length > 0

    - name: Verify NVIDIA driver packages are held
      shell: dpkg --get-selections | grep -E 'nvidia.*-{{ nvidia_driver_version }}' | grep hold
      register: nvidia_held_packages
      changed_when: false
      failed_when: nvidia_held_packages.rc != 0

    - name: Log NVIDIA driver version locking
      shell: |
        echo "ðŸ”’ Version Locking: {{ nvidia_held_packages.stdout_lines | length }} NVIDIA driver packages held" >> {{ log_file }}
        echo "Packages will NOT upgrade with 'apt upgrade'" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - debug:
        msg: "ðŸ”’ {{ nvidia_held_packages.stdout_lines | length }} NVIDIA driver packages held to prevent upgrades"

    # ==========================================================
    # 11. CUDA TOOLKIT (LOCAL REPO)
    # ==========================================================
    - name: Install CUDA Toolkit from local repository
      block:
        - name: Download CUDA local repository package
          get_url:
            url: "{{ cuda_local_repo_url }}"
            dest: "{{ installers_base_path }}/cuda/cuda-repo-local.deb"
            mode: '0644'
            timeout: 600
          register: cuda_download

        - name: Calculate SHA-256 checksum of CUDA package
          shell: sha256sum {{ installers_base_path }}/cuda/cuda-repo-local.deb | awk '{print $1}'
          register: cuda_sha256
          changed_when: false
          when: cuda_download.changed

        - name: Log CUDA SHA-256 checksum
          shell: |
            echo "CUDA Toolkit SHA-256: {{ cuda_sha256.stdout }}" >> {{ log_file }}
            echo "  File: cuda-repo-local.deb" >> {{ log_file }}
            echo "  Downloaded: {{ ansible_date_time.iso8601 }}" >> {{ log_file }}
            echo "{{ cuda_sha256.stdout }}  cuda/cuda-repo-local.deb" >> {{ installers_base_path }}/SHA256SUMS
          when: cuda_download.changed

        - name: Install CUDA local repository package
          apt:
            deb: "{{ installers_base_path }}/cuda/cuda-repo-local.deb"
            state: present

        - name: Find CUDA repository directory
          shell: find /var -maxdepth 1 -type d -name 'cuda-repo-ubuntu2404-*-local' | head -n 1
          register: cuda_repo_dir
          changed_when: false

        - name: Copy CUDA GPG key to keyrings
          shell: cp {{ cuda_repo_dir.stdout }}/cuda-*-keyring.gpg /usr/share/keyrings/
          args:
            creates: /usr/share/keyrings/cuda-*-keyring.gpg
          when: cuda_repo_dir.stdout != ""

        - name: Update apt cache with CUDA repository
          apt:
            update_cache: yes

        - name: Install CUDA toolkit package
          apt:
            name: "cuda-toolkit-{{ cuda_version }}"
            state: present
            update_cache: yes

        - name: Configure CUDA environment in /etc/profile.d/
          copy:
            dest: /etc/profile.d/cuda.sh
            mode: '0644'
            content: |
              export PATH=/usr/local/cuda-{{ cuda_version_full }}/bin:$PATH
              export LD_LIBRARY_PATH=/usr/local/cuda-{{ cuda_version_full }}/lib64:$LD_LIBRARY_PATH

        - name: Configure CUDA library path for ldconfig
          copy:
            dest: /etc/ld.so.conf.d/cuda.conf
            mode: '0644'
            content: |
              /usr/local/cuda-{{ cuda_version_full }}/lib64
          notify: update ldconfig

        - shell: echo "âœ… CUDA {{ cuda_version_full }} installed from local repository." >> {{ log_file }}
        - debug: msg="âœ… CUDA {{ cuda_version_full }} installed from local repository."
      rescue:
        - debug: msg="âš ï¸ CUDA installation failed."
        - shell: echo "âš ï¸ CUDA installation failed." >> {{ log_file }}

    - name: Log CUDA installation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "11. CUDA TOOLKIT - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "Installed CUDA {{ cuda_version_full }} from local repository" >> {{ log_file }}
        echo "Stored in: {{ installers_base_path }}/cuda/" >> {{ log_file }}
        echo "" >> {{ log_file }}

    # Hold CUDA packages to prevent upgrades
    - name: Get list of installed CUDA 13-0 packages
      shell: dpkg -l | grep -E '^ii' | awk '{print $2}' | grep 'cuda.*-13-0$'
      register: installed_cuda_packages
      changed_when: false
      failed_when: false

    - name: Hold installed CUDA packages to prevent upgrades
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop: "{{ installed_cuda_packages.stdout_lines }}"
      when: installed_cuda_packages.stdout_lines | length > 0

    - name: Get list of all installed CUDA packages for holding
      shell: dpkg -l | grep '^ii' | grep -E 'cuda-|libcu' | grep '{{ cuda_version }}' | awk '{print $2}'
      register: additional_cuda_packages
      changed_when: false
      failed_when: false

    - name: Hold additional CUDA packages
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop: "{{ additional_cuda_packages.stdout_lines }}"
      when: additional_cuda_packages.stdout_lines | length > 0

    - name: Verify CUDA packages are held
      shell: dpkg --get-selections | grep -E 'cuda-|libcu' | grep '{{ cuda_version }}' | grep hold
      register: cuda_held_packages
      changed_when: false
      failed_when: false

    - name: Log CUDA version locking
      shell: |
        echo "ðŸ”’ Version Locking: {{ cuda_held_packages.stdout_lines | length }} CUDA packages held" >> {{ log_file }}
        echo "Packages will NOT upgrade with 'apt upgrade'" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - debug:
        msg: "ðŸ”’ {{ cuda_held_packages.stdout_lines | length }} CUDA packages held to prevent upgrades"

    # ==========================================================
    # 12. TENSORRT
    # ==========================================================
    - name: Install TensorRT with version pinning
      block:
        # Detect Ubuntu version and architecture
        - name: Get Ubuntu codename
          shell: . /etc/os-release && echo "$VERSION_CODENAME"
          register: ubuntu_codename
          changed_when: false

        - name: Get system architecture
          command: dpkg --print-architecture
          register: system_arch
          changed_when: false

        - name: Construct TensorRT local repo URL automatically
          set_fact:
            tensorrt_auto_url: "https://developer.download.nvidia.com/compute/tensorrt/{{ tensorrt_version_full }}/local_installers/nv-tensorrt-local-repo-ubuntu{{ ansible_distribution_version | replace('.', '') }}-{{ tensorrt_version_full }}-cuda-{{ cuda_version_full }}_1.0-1_{{ system_arch.stdout }}.deb"

        - name: Set final TensorRT URL
          set_fact:
            tensorrt_final_url: "{{ tensorrt_local_repo_url if tensorrt_local_repo_url != '' else tensorrt_auto_url }}"

        - name: Display TensorRT installation plan
          debug:
            msg: |
              TensorRT Installation Configuration:
              - Method: {{ tensorrt_install_method }}
              - Version: {{ tensorrt_version_full }}
              - CUDA: {{ cuda_version_full }}
              - Ubuntu: {{ ubuntu_codename.stdout }}
              - Architecture: {{ system_arch.stdout }}
              {% if tensorrt_install_method == 'local' %}
              - Local repo URL: {{ tensorrt_final_url }}
              {% endif %}

        # Method 1: Local Repository
        - name: Install TensorRT from local repository
          when: tensorrt_install_method == 'local' or tensorrt_install_method == 'auto'
          block:
            - name: Download TensorRT local repo package
              get_url:
                url: "{{ tensorrt_final_url }}"
                dest: /tmp/tensorrt-local-repo.deb
                timeout: 300
              register: tensorrt_download
              failed_when: false

            - name: Install TensorRT local repo package
              apt:
                deb: /tmp/tensorrt-local-repo.deb
              when: tensorrt_download is succeeded

            - name: Copy TensorRT repo keyring
              shell: cp /var/nv-tensorrt-local-repo-*/tensorrt-*-keyring.gpg /usr/share/keyrings/ 2>/dev/null || cp /var/nv-tensorrt-local-repo-*/*-keyring.gpg /usr/share/keyrings/
              when: tensorrt_download is succeeded
              register: keyring_copy
              failed_when: false

            - name: Update apt cache for TensorRT local repo
              apt:
                update_cache: yes
              when: tensorrt_download is succeeded

            - name: Install TensorRT from local repo
              apt:
                name:
                  - tensorrt-dev
                  - tensorrt-libs
                  - python3-libnvinfer
                  - python3-libnvinfer-dev
                state: present
              when: tensorrt_download is succeeded
              register: trt_local_install

        # Method 2: Network Repository (when local fails or method is "network")
        - name: Install TensorRT from network repository
          when: (tensorrt_install_method == 'network') or (tensorrt_install_method == 'auto' and (tensorrt_download is failed or trt_local_install is failed))
          block:
            - name: Update apt cache for TensorRT
              apt:
                update_cache: yes

            - name: Check available TensorRT versions in network repo
              shell: apt-cache madison tensorrt-dev | grep "{{ tensorrt_version_pattern }}" | head -1 | awk '{print $3}'
              register: available_trt_version
              failed_when: false
              changed_when: false

            - name: Fail if requested TensorRT version not available
              fail:
                msg: |
                  TensorRT {{ tensorrt_version_pattern }} not found in network repository!

                  When newer TensorRT versions are released, NVIDIA removes old versions from the network repo.

                  SOLUTION: Set tensorrt_install_method to "local" or "auto" to use local repository.
                  The playbook will automatically construct the correct URL:
                  {{ tensorrt_final_url }}

                  Or download manually from: https://developer.nvidia.com/tensorrt
              when: available_trt_version.stdout == ''

            - name: Set TensorRT package version from network repo
              set_fact:
                trt_pkg_version: "{{ available_trt_version.stdout }}"

            - name: Install TensorRT packages with specific version from network repo
              apt:
                name:
                  - "tensorrt-dev={{ trt_pkg_version }}"
                  - "tensorrt-libs={{ trt_pkg_version }}"
                  - "libnvinfer10={{ trt_pkg_version }}"
                  - "libnvinfer-dev={{ trt_pkg_version }}"
                  - "libnvinfer-headers-dev={{ trt_pkg_version }}"
                  - "libnvinfer-plugin10={{ trt_pkg_version }}"
                  - "libnvonnxparsers10={{ trt_pkg_version }}"
                  - "python3-libnvinfer={{ trt_pkg_version }}"
                  - "python3-libnvinfer-dev={{ trt_pkg_version }}"
                state: present
                allow_downgrades: yes
              register: trt_install

        - name: Hold TensorRT packages to prevent upgrades
          dpkg_selections:
            name: "{{ item }}"
            selection: hold
          loop:
            - tensorrt-dev
            - tensorrt-libs
            - libnvinfer10
            - libnvinfer-dev
            - libnvinfer-headers-dev
            - libnvinfer-plugin10
            - libnvonnxparsers10
            - python3-libnvinfer
            - python3-libnvinfer-dev

        - name: Verify packages are held
          shell: dpkg --get-selections | grep -E "(tensorrt|libnvinfer)" | grep hold
          register: held_packages
          changed_when: false
          failed_when: held_packages.rc != 0

        - name: Verify installed TensorRT version
          shell: dpkg -l | grep -E "(tensorrt|libnvinfer)" | grep -v "^rc"
          register: trt_ver
          changed_when: false

        - shell: |
            echo "{{ trt_ver.stdout }}" >> {{ log_file }}
            echo "" >> {{ log_file }}
            echo "Packages held (will NOT be upgraded by apt):" >> {{ log_file }}
            echo "{{ held_packages.stdout }}" >> {{ log_file }}

        - name: Determine which method was used
          set_fact:
            trt_method_used: "{{ 'Local Repository' if (tensorrt_download is defined and tensorrt_download is succeeded) else 'Network Repository' }}"

        - debug:
            msg: |
              âœ… TensorRT {{ tensorrt_version_full }} installed and pinned
              Installation method: {{ trt_method_used }}
              {% if trt_method_used == 'Local Repository' %}
              Local repo URL: {{ tensorrt_final_url }}
              {% endif %}

              Held packages (protected from apt upgrade):
              {{ held_packages.stdout_lines }}

    - name: Log TensorRT installation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "12. TENSORRT - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "" >> {{ log_file }}

    # ==========================================================
    # 13. UNATTENDED-UPGRADES PROTECTION
    # ==========================================================
    - name: Create unattended-upgrades GPU stack blacklist
      copy:
        dest: /etc/apt/apt.conf.d/51nvidia-blacklist
        mode: '0644'
        content: |
          // Prevent automatic upgrades of GPU stack packages
          // This ensures the frozen NVIDIA driver + CUDA + TensorRT versions remain stable
          Unattended-Upgrade::Package-Blacklist {
              "nvidia-driver-*";
              "nvidia-utils-*";
              "libnvidia-*";
              "nvidia-kernel-*";
              "nvidia-dkms-*";
              "cuda-*";
              "libcu*";
              "tensorrt-*";
              "libnvinfer*";
              "libnvonnxparsers*";
          };

    - name: Log unattended-upgrades protection
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "13. UNATTENDED-UPGRADES PROTECTION - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "Created GPU stack blacklist in /etc/apt/apt.conf.d/51nvidia-blacklist" >> {{ log_file }}
        echo "GPU packages will NOT be automatically upgraded" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - debug:
        msg: "ðŸ›¡ï¸ Unattended-upgrades GPU stack blacklist created"

    # ==========================================================
    # 14. APT PREFERENCES (PIN-PRIORITY 1001)
    # ==========================================================
    - name: Create APT preferences for GPU stack pinning
      copy:
        dest: /etc/apt/preferences.d/99-gpu-stack-pinning
        mode: '0644'
        content: |
          # Pin NVIDIA Driver {{ nvidia_driver_version_full }} - Highest Priority
          Package: nvidia-driver-{{ nvidia_driver_version }}
          Pin: version {{ nvidia_driver_version_full }}*
          Pin-Priority: 1001

          Package: nvidia-*-{{ nvidia_driver_version }}
          Pin: version *
          Pin-Priority: 1001

          # Pin CUDA Toolkit {{ cuda_version_full }}
          Package: cuda-toolkit-{{ cuda_version }}
          Pin: version *
          Pin-Priority: 1001

          Package: cuda-*-{{ cuda_version }}
          Pin: version *
          Pin-Priority: 1001

          Package: libcu*
          Pin: version *{{ cuda_version_full }}*
          Pin-Priority: 1001

          # Pin TensorRT {{ tensorrt_version_full }}
          Package: tensorrt-*
          Pin: version *{{ tensorrt_version_full }}*
          Pin-Priority: 1001

          Package: libnvinfer*
          Pin: version *
          Pin-Priority: 1001

          # Prevent newer CUDA versions from being installed
          Package: cuda-toolkit-14-*
          Pin: version *
          Pin-Priority: -1

          Package: cuda-toolkit-15-*
          Pin: version *
          Pin-Priority: -1

    - name: Log APT preferences creation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "14. APT PREFERENCES PINNING - {{ ansible_date_time.iso8601 }}" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "Created /etc/apt/preferences.d/99-gpu-stack-pinning" >> {{ log_file }}
        echo "Pin-Priority 1001 for nvidia-{{ nvidia_driver_version }}, cuda-{{ cuda_version }}, tensorrt-{{ tensorrt_version_full }}" >> {{ log_file }}
        echo "Fourth layer of protection (beyond dpkg hold + unattended-upgrades)" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - debug:
        msg: "ðŸ“Œ APT preferences pinning created (Pin-Priority 1001)"

    # ==========================================================
    # 15. REPOSITORY CLEANUP
    # ==========================================================
    - name: Clean up old local repositories
      shell: |
        # Keep only the most recent TensorRT local repo
        find /var -maxdepth 1 -mindepth 1 -type d -name 'nv-tensorrt-local-repo-*' 2>/dev/null | sort -r | tail -n +2 | xargs -r rm -rf 2>/dev/null || true
        # Keep only the most recent CUDA local repo
        find /var -maxdepth 1 -mindepth 1 -type d -name 'cuda-repo-ubuntu2404-*-local' 2>/dev/null | sort -r | tail -n +2 | xargs -r rm -rf 2>/dev/null || true
        # Keep only the most recent NVIDIA driver local repo
        find /var -maxdepth 1 -mindepth 1 -type d -name 'nvidia-driver-local-repo-*' 2>/dev/null | sort -r | tail -n +2 | xargs -r rm -rf 2>/dev/null || true
      failed_when: false
      changed_when: false

    - name: Log repository cleanup
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "15. REPOSITORY CLEANUP - {{ ansible_date_time.iso8601 }}" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "Cleaned up old local repositories (kept most recent only)" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - debug:
        msg: "ðŸ§¹ Old local repositories cleaned up"

    # ==========================================================
    # 15. DOCKER + NVIDIA RUNTIME
    # ==========================================================
    - name: Install Docker and NVIDIA container runtime
      block:
        - name: Install Docker prerequisites
          apt:
            name:
              - apt-transport-https
              - ca-certificates
              - curl
              - gnupg
              - lsb-release
            state: present

        - name: Create keyrings directory
          file:
            path: /etc/apt/keyrings
            state: directory
            mode: '0755'

        - name: Download Docker GPG key directly to keyrings
          shell: curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
          args:
            creates: /etc/apt/keyrings/docker.asc

        - name: Set permissions on Docker GPG key
          file:
            path: /etc/apt/keyrings/docker.asc
            mode: '0644'

        - name: Add Docker repository with dynamic architecture and codename
          shell: |
            echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
            $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
            tee /etc/apt/sources.list.d/docker.list > /dev/null
          args:
            creates: /etc/apt/sources.list.d/docker.list

        - name: Update apt cache for Docker
          apt:
            update_cache: yes

        - name: Install Docker packages
          apt:
            name:
              - docker-ce
              - docker-ce-cli
              - containerd.io
              - docker-buildx-plugin
              - docker-compose-plugin
            state: present

        - name: Add user to docker group
          user:
            name: "{{ ssh_user }}"
            groups: docker
            append: yes

        - name: Create keyrings directory for NVIDIA Container Toolkit
          file:
            path: /usr/share/keyrings
            state: directory
            mode: '0755'

        - name: Download and install NVIDIA Container Toolkit GPG key
          shell: |
            curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
            gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
          args:
            creates: /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

        - name: Add NVIDIA Container Toolkit repository
          shell: |
            curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
            tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
          args:
            creates: /etc/apt/sources.list.d/nvidia-container-toolkit.list

        - name: Update apt cache for NVIDIA Container Toolkit
          apt:
            update_cache: yes

        - name: Install NVIDIA Container Toolkit
          apt:
            name: nvidia-container-toolkit
            state: present

        - name: Configure Docker runtime using nvidia-ctk
          command: nvidia-ctk runtime configure --runtime=docker
          register: nvidia_ctk_config
          changed_when: nvidia_ctk_config.rc == 0
          notify: restart docker

        - name: Start and enable Docker service
          systemd:
            name: docker
            state: started
            enabled: yes

        - shell: echo "âœ… Docker and NVIDIA Container Toolkit installed." >> {{ log_file }}
        - debug: msg="âœ… Docker and NVIDIA Container Toolkit installed."
      rescue:
        - debug: msg="âš ï¸ Docker installation failed."
        - shell: echo "âš ï¸ Docker installation failed." >> {{ log_file }}
    
    - name: Log Docker installation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "13. DOCKER + NVIDIA RUNTIME - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "" >> {{ log_file }}

    # ==========================================================
    # 14. PYTHON & DEV TOOLS
    # ==========================================================
    - name: Install Python and dev tools
      apt:
        name:
          - python3
          - python3-pip
          - python3-venv
          - python3.12-venv
          - python3-dev
        state: present
      register: python_install
    
    - shell: python3 --version
      register: pyver
    
    - name: Log Python installation
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "14. PYTHON & DEV TOOLS - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "âœ… Python {{ pyver.stdout }} installed." >> {{ log_file }}
        echo "" >> {{ log_file }}
    
    - debug: msg="âœ… Python {{ pyver.stdout }} installed."

    # ==========================================================
    # 15. HEALTHCHECKS.IO SERVICE
    # ==========================================================
    - name: Setup Healthchecks.io service
      block:
        - name: Create healthcheck ping script
          copy:
            dest: /usr/local/bin/healthcheck-ping.sh
            mode: '0755'
            content: |
              #!/bin/bash
              set -e
              curl -fsS -m 10 --retry 5 -o /dev/null "{{ healthchecks_url }}" || echo "Healthcheck failed $(date)"
        - name: Create systemd service for healthcheck
          copy:
            dest: /etc/systemd/system/healthcheck.service
            mode: '0644'
            content: |
              [Unit]
              Description=Healthchecks.io ping

              [Service]
              Type=oneshot
              ExecStart=/usr/local/bin/healthcheck-ping.sh
        - name: Create systemd timer for healthcheck (every 5 minutes)
          copy:
            dest: /etc/systemd/system/healthcheck.timer
            mode: '0644'
            content: |
              [Unit]
              Description=Run healthcheck ping every 5 minutes

              [Timer]
              OnBootSec=1min
              OnUnitActiveSec=5min
              AccuracySec=30s

              [Install]
              WantedBy=timers.target
          notify: reload systemd
        - name: Enable and start healthcheck timer
          systemd: { name: healthcheck.timer, enabled: yes, state: started }
          register: healthcheck_timer

        - name: Log healthcheck configuration
          shell: |
            echo "===========================================================" >> {{ log_file }}
            echo "15. HEALTHCHECKS.IO SERVICE - $(date)" >> {{ log_file }}
            echo "===========================================================" >> {{ log_file }}
            echo "âœ… Healthcheck service active (ping every 5 min)." >> {{ log_file }}
            echo "URL: {{ healthchecks_url }}" >> {{ log_file }}
            echo "" >> {{ log_file }}

        - debug: msg="âœ… Healthcheck service active (ping every 5 min) - {{ healthchecks_url }}"
      when: healthchecks_url != 'SKIPPED'

    - name: Log healthcheck skipped
      block:
        - shell: |
            echo "===========================================================" >> {{ log_file }}
            echo "15. HEALTHCHECKS.IO SERVICE - $(date)" >> {{ log_file }}
            echo "===========================================================" >> {{ log_file }}
            echo "âš ï¸  Healthcheck service skipped (no URL provided)." >> {{ log_file }}
            echo "" >> {{ log_file }}

        - debug: msg="âš ï¸  Healthcheck service skipped (no URL provided)."
      when: healthchecks_url == 'SKIPPED'

    # ==========================================================
    # 16. AUTO_TEST PYTHON ENVIRONMENT SETUP
    # ==========================================================
    - name: Create code directory structure
      file:
        path: "{{ user_home }}/code/auto_test"
        state: directory
        mode: '0755'
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"

    - name: Create Python 3.12 virtual environment
      become: yes
      become_user: "{{ ssh_user }}"
      command: python3.12 -m venv "{{ user_home }}/code/auto_test/venv"
      args:
        creates: "{{ user_home }}/code/auto_test/venv"

    - name: Upgrade pip, setuptools, and wheel in venv
      become: yes
      become_user: "{{ ssh_user }}"
      shell: |
        source {{ user_home }}/code/auto_test/venv/bin/activate
        pip install --upgrade pip setuptools wheel
      args:
        executable: /bin/bash

    - name: Create activation helper script
      copy:
        dest: "{{ user_home }}/code/auto_test/activate.sh"
        mode: '0755'
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"
        content: |
          #!/bin/bash
          source {{ user_home }}/code/auto_test/venv/bin/activate
          echo "Virtual environment activated!"
          echo "Python: $(which python3)"
          python3 << 'EOF'
          import torch
          print(f"PyTorch {torch.__version__} - CUDA available: {torch.cuda.is_available()}")
          if torch.cuda.is_available():
              print(f"GPU: {torch.cuda.get_device_name(0)}")
          EOF

    - name: Log auto_test environment setup
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "16. AUTO_TEST PYTHON ENVIRONMENT SETUP - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "âœ… Created ~/code/auto_test directory" >> {{ log_file }}
        echo "âœ… Python 3.12 virtual environment created" >> {{ log_file }}
        echo "" >> {{ log_file }}
        echo "âš ï¸  ML packages (PyTorch, ultralytics, etc.) will be installed after reboot" >> {{ log_file }}
        echo "âš ï¸  Run post-reboot-verify.yml after reboot to install and verify" >> {{ log_file }}
        echo "" >> {{ log_file }}

    - debug:
        msg: |
          âœ… Virtual environment created at ~/code/auto_test

          âš ï¸  Reboot required to load NVIDIA driver
          âš ï¸  After reboot, run: ansible-playbook post-reboot-verify.yml -K
          This will install PyTorch, ultralytics, TensorRT, and verify CUDA

    # ==========================================================
    # 17. REBOOT NOTIFICATION
    # ==========================================================
    - name: Check if reboot is required
      stat:
        path: /var/run/reboot-required
      register: reboot_required_file

    - name: Set reboot requirement fact
      set_fact:
        needs_reboot: "{{ reboot_required_file.stat.exists or nvidia_driver_install.changed }}"

    - name: Log reboot requirement
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "17. REBOOT NOTIFICATION - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        {% if needs_reboot %}
        echo "âš ï¸  REBOOT REQUIRED" >> {{ log_file }}
        echo "NVIDIA driver and/or kernel modules have been updated." >> {{ log_file }}
        echo "Please reboot to activate all changes: sudo reboot" >> {{ log_file }}
        {% else %}
        echo "âœ… No reboot required at this time." >> {{ log_file }}
        {% endif %}
        echo "" >> {{ log_file }}

    - name: Display reboot notification
      debug:
        msg: |
          âš ï¸  ============================================
          âš ï¸  REBOOT REQUIRED
          âš ï¸  ============================================
          NVIDIA driver and/or kernel modules have been updated.
          The system needs to be rebooted to activate all changes.

          Please reboot when convenient:
            sudo reboot
      when: needs_reboot

    # ==========================================================
    # 17. GIT SETUP (SSH default, GitHub key â€“ prompted at end of run)
    # ==========================================================
    - name: Prompt for Git user name
      pause:
        prompt: "Git user.name for commits on this machine"
      register: git_name_prompt
      when: git_user_name is not defined

    - name: Set git_user_name from prompt
      set_fact:
        git_user_name: "{{ git_name_prompt.user_input | default('', true) | trim }}"
      when: git_user_name is not defined

    - name: Prompt for Git user email
      pause:
        prompt: "Git user.email for commits on this machine"
      register: git_email_prompt
      when: git_user_email is not defined

    - name: Set git_user_email from prompt
      set_fact:
        git_user_email: "{{ git_email_prompt.user_input | default('', true) | trim }}"
      when: git_user_email is not defined

    - name: Set git user.name and user.email
      command: 'git config --global {{ item.key | quote }} {{ item.value | quote }}'
      loop:
        - { key: "user.name", value: "{{ git_user_name | default('') }}" }
        - { key: "user.email", value: "{{ git_user_email | default('') }}" }
      when: (git_user_name | default('')) | length > 0 and (git_user_email | default('')) | length > 0
      become_user: "{{ ssh_user }}"
      changed_when: true

    - name: Set Git to use SSH instead of HTTPS for GitHub
      command: git config --global url."git@github.com:".insteadOf "https://github.com/"
      become_user: "{{ ssh_user }}"
      changed_when: true

    - name: Ensure .ssh exists for GitHub key
      file:
        path: "{{ user_home }}/.ssh"
        state: directory
        mode: '0700'
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"

    - name: Generate SSH key for GitHub (ed25519)
      command: >
        ssh-keygen -t ed25519 -C "github-{{ ansible_hostname }}"
        -f {{ user_home }}/.ssh/id_ed25519_github -N ""
      args:
        creates: "{{ user_home }}/.ssh/id_ed25519_github"
      become_user: "{{ ssh_user }}"

    - name: Ensure SSH config uses GitHub key for github.com
      blockinfile:
        path: "{{ user_home }}/.ssh/config"
        create: true
        mode: '0600'
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"
        block: |
          Host github.com
            IdentityFile {{ user_home }}/.ssh/id_ed25519_github
        marker: "# {mark} ANSIBLE MANAGED BLOCK - GitHub key"
      become_user: "{{ ssh_user }}"

    - name: Read GitHub SSH public key
      slurp:
        src: "{{ user_home }}/.ssh/id_ed25519_github.pub"
      register: github_key_content

    - name: Display GitHub SSH public key for user to add
      debug:
        msg: |
          ========== ADD THIS KEY TO GITHUB ==========
          Settings â†’ SSH and GPG keys â†’ New SSH key
          Paste the key below, then press Enter in the playbook to continue.
          ~/.ssh/config is set so git will use this key (no ssh-agent needed).
          ----------
          {{ github_key_content.content | b64decode | trim }}
          ----------
      when: github_key_content.content is defined

    - name: Pause for user to add key to GitHub
      pause:
        prompt: "Add the key above to GitHub, then press Enter to continue"
      when: github_key_content.content is defined

    - name: Log Git setup
      shell: |
        echo "===========================================================" >> {{ log_file }}
        echo "17. GIT SETUP - $(date)" >> {{ log_file }}
        echo "===========================================================" >> {{ log_file }}
        echo "âœ… Git configured (SSH for GitHub); key in ~/.ssh/id_ed25519_github" >> {{ log_file }}
        echo "" >> {{ log_file }}

    # ==========================================================
    # 18. FINAL MESSAGE
    # ==========================================================
    - name: Write final summary to log
      shell: |
        echo "===================================================================" >> {{ log_file }}
        echo "âœ… Setup complete!" >> {{ log_file }}
        echo "- NVIDIA driver {{ nvidia_driver_version }}, CUDA {{ cuda_version_full }}" >> {{ log_file }}
        echo "- TensorRT installed" >> {{ log_file }}
        echo "- SSH on port {{ ssh_port }} (key-only)" >> {{ log_file }}
        echo "- Docker + NVIDIA runtime ready" >> {{ log_file }}
        echo "- Tailscale and RealVNC installed" >> {{ log_file }}
        echo "- Git configured (SSH for GitHub)" >> {{ log_file }}
        {% if healthchecks_url != 'SKIPPED' %}
        echo "- Healthcheck active (every 5 min): {{ healthchecks_url }}" >> {{ log_file }}
        {% else %}
        echo "- Healthcheck skipped (no URL provided)" >> {{ log_file }}
        {% endif %}
        echo "- Scheduled reboots at 6 and 18 (root cron installed by playbook)" >> {{ log_file }}
        echo "- Auto test environment: ~/code/auto_test (Python 3.12 venv)" >> {{ log_file }}
        echo "===================================================================" >> {{ log_file }}
        echo "Completed at: $(date)" >> {{ log_file }}
        echo "Log file: {{ log_file }}" >> {{ log_file }}
        {% if needs_reboot %}
        echo "" >> {{ log_file }}
        echo "âš ï¸  Please reboot the system: sudo reboot" >> {{ log_file }}
        {% endif %}

    - debug:
        msg: |
          ===================================================================
          âœ… Setup complete!
          - NVIDIA driver {{ nvidia_driver_version }}, CUDA {{ cuda_version_full }}
          - TensorRT installed
          - SSH on port {{ ssh_port }} (key-only)
          - Docker + NVIDIA runtime ready
          - Tailscale and RealVNC installed
          - Git configured (SSH for GitHub)
          {% if healthchecks_url != 'SKIPPED' %}
          - Healthcheck active (every 5 min): {{ healthchecks_url }}
          {% else %}
          - Healthcheck skipped (no URL provided)
          {% endif %}
          - Scheduled reboots at 6 and 18 (root cron installed by playbook)
          - Auto test environment: ~/code/auto_test (Python 3.12 venv)

          Activate venv: source ~/code/auto_test/activate.sh
          Log file saved to: {{ log_file }}
          {% if needs_reboot %}

          âš ï¸  Please reboot the system: sudo reboot
          {% endif %}
          ===================================================================

  handlers:
    - name: restart sshd
      systemd: { name: sshd, state: restarted }
    - name: restart docker
      systemd: { name: docker, state: restarted }
    - name: restart gdm3
      systemd: { name: gdm3, state: restarted }
      when: gdm3_config.stat.exists | default(false)
    - name: restart lightdm
      systemd: { name: lightdm, state: restarted }
      when: lightdm_config.stat.exists | default(false)
    - name: update ldconfig
      command: ldconfig
    - name: reload systemd
      systemd: { daemon_reload: yes }
